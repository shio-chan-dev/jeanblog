+++
title = "Attention"
description = "Self-attention 与 Cross-attention 机制解析"
+++

## 推荐阅读

- 先读注意力机制公式与直观解释
- 再看 Self-Attention/Transformer 结构
- 最后看多头、稀疏与线性注意力变体
